{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reddit Depression Detection Final Project\n",
        "Link to the paper: https://dl.acm.org/doi/pdf/10.1145/3578503.3583621\n",
        "\n",
        "## Project explanation\n",
        "- Explore specific 13 symptoms of depression through languaged gathered from Reddit subreddits.\n",
        "- Goal: Produce models that can detect symptoms in text. Could be helpful with early detection of mental health issues.\n",
        "- Use LDA- topic distribution and Roberta model to extract deeper language patterns to help predict symptoms in text data.\n",
        "- Use forest classifier for evaluation of models."
      ],
      "metadata": {
        "id": "9jFvbbC6VtZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations and Imports"
      ],
      "metadata": {
        "id": "cjovb7aOErFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install happiestfuntokenizing\n",
        "!pip install transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eFM7SOQVOAK7",
        "outputId": "db60fa5b-5be9-480f-df1f-629fe3681a6c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: happiestfuntokenizing in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "FoBxKQ_OVl-j"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim import corpora\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import RobertaTokenizer, DistilBertModel\n",
        "import torch\n",
        "\n",
        "from tabulate import tabulate\n",
        "\n",
        "from sklearn.model_selection import cross_validate, cross_val_score, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from happiestfuntokenizing.happiestfuntokenizing import Tokenizer\n",
        "\n",
        "# IF USING COLAB DRIVE\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#FILEPATH = '/content/drive/MyDrive/Brown University/Coursework/Fall 2024/CSCI 1460- Computational Linguistics/Final Project/student.pkl'\n",
        "\n",
        "# ADJUST FILE PATHS\n",
        "FILEPATH = '/content/student.pkl'\n",
        "ROBERTA_FILEPATH = '/content/roberta_batch_embeddings.pkl'\n",
        "LDA_FILEPATH = '/content/lda_feature_embeddings.pkl'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "rcMOTL7mV9T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "  \"\"\"Load pickles\"\"\"\n",
        "  df = pd.read_pickle(FILEPATH)\n",
        "  return df\n",
        "\n",
        "def load_embeddings():\n",
        "  roberta_embeddings = pd.read_pickle(ROBERTA_FILEPATH)\n",
        "  lda_embeddings = pd.read_pickle(LDA_FILEPATH)\n",
        "  return roberta_embeddings, lda_embeddings"
      ],
      "metadata": {
        "id": "icnzto8GWFlb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of depression subreddits in the paper\n",
        "depression_subreddits = [\"Anger\",\n",
        "    \"anhedonia\", \"DeadBedrooms\",\n",
        "    \"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\",\n",
        "    \"DecisionMaking\", \"shouldi\",\n",
        "    \"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\",\n",
        "    \"chronicfatigue\", \"Fatigue\",\n",
        "    \"ForeverAlone\", \"lonely\",\n",
        "    \"cry\", \"grief\", \"sad\", \"Sadness\",\n",
        "    \"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\",\n",
        "    \"insomnia\", \"sleep\",\n",
        "    \"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\",\n",
        "    \"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\",\n",
        "    \"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"\n",
        "]"
      ],
      "metadata": {
        "id": "ohOK3wCdWpnA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_generation():\n",
        "  \"\"\"Build control and symptom datasets\"\"\"\n",
        "  df = load_data()\n",
        "\n",
        "  # Data split into \"13\" symptoms\n",
        "  anger_data = df[df['subreddit'].isin(['Anger'])]\n",
        "  anhedonia_data = df[df['subreddit'].isin(['anhedonia', 'DeadBedrooms'])]\n",
        "  anxiety_data = df[df['subreddit'].isin(['Anxiety', 'AnxietyDepression', 'HealthAnxiety', 'PanicAttack'])]\n",
        "  concen_deficit_data = df[df['subreddit'].isin(['DecisionMaking', 'shouldi'])]\n",
        "  disordered_eating_data = df[df['subreddit'].isin(['bingeeating', 'BingeEatingDisorder', 'EatingDisorders', 'eating_disorders', 'EDAnonymous'])]\n",
        "  fatigue_data = df[df['subreddit'].isin(['chronicfatigue', 'Fatigue'])]\n",
        "  loneliness_data = df[df['subreddit'].isin(['ForeverAlone', 'lonely'])]\n",
        "  sad_mood_data = df[df['subreddit'].isin(['cry', 'grief', 'sad', 'Sadness'])]\n",
        "  self_loathing = df[df['subreddit'].isin(['AvPD', 'SelfHate', 'selfhelp', 'socialanxiety', 'whatsbotheringyou'])]\n",
        "  sleep_problem_data = df[df['subreddit'].isin(['insomnia', 'sleep'])]\n",
        "  somatic_data = df[df['subreddit'].isin(['cfs', 'ChronicPain', 'Constipation', 'EssentialTremor', 'headaches', 'ibs', 'tinnitus'])]\n",
        "  suicidal_thoughts_data = df[df['subreddit'].isin(['AdultSelfHarm', 'selfharm', 'SuicideWatch'])]\n",
        "  worthlessness_data = df[df['subreddit'].isin(['Guilt', 'Pessimism'])]\n",
        "\n",
        "  # Full Depression Data set including all Symptoms\n",
        "  depression_data = df[df['subreddit'].isin(depression_subreddits)]\n",
        "\n",
        "  # Control Data- Idea from https://tinyurl.com/ControlData\n",
        "  # \"Only [keep] non-mental health posts by authors that were at least 180â€‰days\n",
        "  # older than their index (earliest) post in a mental health subreddit\"\n",
        "\n",
        "  # Create DF that links authors to their earlist post on the depression_data\n",
        "  earliest_post = (\n",
        "      depression_data.groupby('author')['created_utc'] # groups author with utc time\n",
        "      .min() # earliest value (smallest utc for that author)\n",
        "      .reset_index() # turns it into an indexed df\n",
        "      .rename(columns={'created_utc': 'earliest_mental_health_post'}) # renames columns\n",
        "  )\n",
        "\n",
        "  # Merge new df that includes earliest post with original df on the author name\n",
        "  control_df = df.merge(earliest_post, on='author', how='left')\n",
        "\n",
        "  SECONDS_IN_180_DAYS = 180 * 24 * 60 * 60 # 180 days in seconds\n",
        "\n",
        "  control_data = control_df[\n",
        "    (~control_df['subreddit'].isin(depression_subreddits)) &  # exclude mental health subreddits\n",
        "    (control_df['created_utc'] < control_df['earliest_mental_health_post'] - SECONDS_IN_180_DAYS)  # at least 180 days before index post\n",
        "  ]\n",
        "\n",
        "  # Our dictionary for all our symptoms, including a Control and all symptoms category (Depression)\n",
        "  symptom_data ={\n",
        "      \"Control\": control_data['text'].tolist(),\n",
        "      \"Depression\": depression_data['text'].tolist(),\n",
        "      \"Anger\": anger_data['text'].tolist(),\n",
        "      \"Anhedonia\": anhedonia_data['text'].tolist(),\n",
        "      \"Anxiety\": anxiety_data['text'].tolist(),\n",
        "      \"Concentration deficit\": concen_deficit_data['text'].tolist(),\n",
        "      \"Disordered Eating\": disordered_eating_data['text'].tolist(),\n",
        "      \"Fatigue\": fatigue_data['text'].tolist(),\n",
        "      \"Loneliness\": loneliness_data['text'].tolist(),\n",
        "      \"Sad Mood\": sad_mood_data['text'].tolist(),\n",
        "      \"Self-loathing\": self_loathing['text'].tolist(),\n",
        "      \"Sleep problem\": sleep_problem_data['text'].tolist(),\n",
        "      \"Somatic complaint\": somatic_data['text'].tolist(),\n",
        "      \"Suicidal thoughts and attempts\": suicidal_thoughts_data['text'].tolist(),\n",
        "      \"Worthlessness\": worthlessness_data['text'].tolist()\n",
        "  }\n",
        "\n",
        "  return symptom_data"
      ],
      "metadata": {
        "id": "Wpw9kJiras4B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST datasorting\n",
        "symptom_data = dataset_generation()\n",
        "print(symptom_data['Control'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsA7E_NF0PaJ",
        "outputId": "8064bc8c-308c-4c9d-f4ca-b1705f95eb6b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Man, I do love me some Bandicoot crash. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA Tokenization"
      ],
      "metadata": {
        "id": "pmHD21JoHXzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(tokenizer, symptom_data):\n",
        "  \"\"\"Tokenize Symptom data using HappyTokenizer for LDA\"\"\"\n",
        "\n",
        "  tokenized_data ={} # dictionary- key: Symptom and value: list[list[token]]\n",
        "\n",
        "  # Loop through all the texts for each symptom\n",
        "  for symptom, texts in tqdm(symptom_data.items(), desc=\"Tokenizing Symptoms\", ncols=100):\n",
        "    tokens_list = []\n",
        "    # Loop through lists of text in each individual symptom\n",
        "    for text in texts:\n",
        "      tokens = tokenizer.tokenize(text) # Tokenize sentence which makes all lower case\n",
        "      if tokens == []: # If sentence is tokenized to be empty, ignore it.\n",
        "        continue\n",
        "      tokens_list.append(tokens) # add tokenized text to list\n",
        "    tokenized_data[symptom] = tokens_list # add tokenized list to dic\n",
        "\n",
        "  return tokenized_data\n",
        "\n",
        "def stop_words(tokenized_data, top_n = 100):\n",
        "  \"\"\"Find top 100 words from Reddit dataset to use as stop words\"\"\"\n",
        "\n",
        "  # Control Vocab\n",
        "  vocab = [] # get all words in our vocab\n",
        "  for sentence in tokenized_data['Control']:\n",
        "    vocab.extend(sentence)\n",
        "\n",
        "  # Count top 100 vocab\n",
        "  word_counts = Counter(vocab)\n",
        "  top_100 = [word for word, count in word_counts.most_common(top_n)]\n",
        "  return top_100\n",
        "\n",
        "def remove_stop_words(tokenized_data, top_100):\n",
        "  \"\"\"Remove stop words from our tokenized data\"\"\"\n",
        "  stop_words = top_100\n",
        "  processed_data = {}\n",
        "\n",
        "  # loop through our tokenized data\n",
        "  for symptom, sentences in tqdm(tokenized_data.items(), desc=\"Removing Stop Words\", ncols=100):\n",
        "    filtered_sentences = []\n",
        "    for sentence in sentences:\n",
        "      # remove all words that were in top 100 stop words\n",
        "      filtered_sentence = [word for word in sentence if word not in stop_words]\n",
        "      filtered_sentences.append(filtered_sentence)\n",
        "    processed_data[symptom] = filtered_sentences\n",
        "\n",
        "  return processed_data"
      ],
      "metadata": {
        "id": "MWGVUju_WxuP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST tokenization\n",
        "\n",
        "happy_tokenizer = Tokenizer() # tokenizer already produce lower case token\n",
        "tokenized_data = tokenize(happy_tokenizer, symptom_data) # tokenize\n",
        "# test tokenization\n",
        "print(tokenized_data['Control'][0:2])\n",
        "\n",
        "# control length\n",
        "print(len(tokenized_data['Control']))\n",
        "print(len(tokenized_data['Depression'])) # depression length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbz0zXKaOqsh",
        "outputId": "004e6f76-a05f-4da7-89a1-60b002309e5e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing Symptoms: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:24<00:00,  9.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['man', ',', 'i', 'do', 'love', 'me', 'some', 'bandicoot', 'crash', '.'], ['how', 'good', 'is', 'this', 'pc', 'for', 'my', '700-750', '$', 'budget', '?', 'want', 'it', 'for', 'gaming', 'on', 'high', '/', 'ultra', 'settings', ',', 'thanks', '!', 'https://www.youtube.com/watch', 'https://www.youtube.com/watch', '?', 'v', '=', 'y_ulqrs', '76xs', '&', 'amp', ';', 't', '=', '110s']]\n",
            "4369\n",
            "94514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST stop word removal\n",
        "\n",
        "# top 100 stop words from control\n",
        "top_100 = stop_words(tokenized_data)\n",
        "print(\"STOP WORDS:\", top_100)\n",
        "\n",
        "# removal of stop words from whole dataset?\n",
        "processed_data = remove_stop_words(tokenized_data, top_100)\n",
        "print()\n",
        "print(processed_data['Control'][0:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPL2byaxb3NM",
        "outputId": "ea1e3c3d-9ff6-4d97-b1aa-7fd9de603692"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STOP WORDS: ['.', ',', 'i', 'the', 'to', 'and', 'a', 'of', '?', 'my', 'in', 'it', 'is', 'for', 'that', '*', 'this', 'but', 'on', 'you', ')', 'with', 'was', 'have', '(', 'me', 'so', 'be', '\"', '-', \"i'm\", 'or', 'just', '[', ']', 'if', 'not', 'what', 'like', '!', 'are', 'as', 'at', '/', ':', 'do', 'about', 'up', 'out', 'can', 'all', 'he', 'from', 'we', 'they', ';', 'her', 'how', 'would', 'she', 'get', 'when', 'one', 'an', 'know', 'had', \"don't\", \"it's\", 'there', 'some', 'been', 'will', 'time', \"i've\", 'any', 'because', 'no', 'more', 'am', 'want', 'your', 'has', 'really', 'people', 'now', 'them', 'amp', '&', 'who', 'other', 'only', 'think', 'by', 'even', 'his', 'back', 'much', '|', 'good', 'then']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Removing Stop Words: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:30<00:00,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[['man', 'love', 'bandicoot', 'crash'], ['pc', '700-750', '$', 'budget', 'gaming', 'high', 'ultra', 'settings', 'thanks', 'https://www.youtube.com/watch', 'https://www.youtube.com/watch', 'v', '=', 'y_ulqrs', '76xs', 't', '=', '110s']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "# IF i wanted to use tokenized data in Roberta, recreate sentences from tokenized lists.\n",
        "print(symptom_data['Control'][0])\n",
        "testing_dic = {}\n",
        "for symptom, posts in processed_data.items():\n",
        "  list_of_setences = []\n",
        "  for post in posts:\n",
        "    sentence = \" \".join(post)\n",
        "    list_of_setences.append(sentence)\n",
        "  testing_dic[symptom] = list_of_setences\n",
        "\n",
        "print(testing_dic['Control'][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwyQGtZVyLbY",
        "outputId": "0b18e31e-7393-4cd7-c2e9-f9cab0f01480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Man, I do love me some Bandicoot crash. \n",
            "man love bandicoot crash\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reddit Topics with LDA\n",
        "\n",
        " - Don't use MALLET (as the paper does), use some other LDA implementation."
      ],
      "metadata": {
        "id": "U4I37U1SXAEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We highly recommend you using the LdaMulticore interface, but feel free to use any other implementations if you prefer.\n",
        "# from gensim.models import LdaMulticore\n",
        "\n",
        "def create_corpus(tokenized_data):\n",
        "  \"\"\"\n",
        "  Produces a dictionary and bow of our data corpus to train our LDA model.\n",
        "  \"\"\"\n",
        "\n",
        "  # Combine our control and all the symptoms dataset\n",
        "  combined_data = tokenized_data['Control'] + tokenized_data['Depression']\n",
        "  # Creates a dictionary mapping for all the posts (uniqueid, word)\n",
        "  id2word = corpora.Dictionary(combined_data)\n",
        "  # Local bow for each post (id, count4post) <-- links uniqueid accross all posts\n",
        "  corpus = [id2word.doc2bow(text) for text in combined_data]\n",
        "\n",
        "  return id2word, corpus\n",
        "\n",
        "def train_lda(id2word, corpus, num_topics=200):\n",
        "\n",
        "  # Produces our LDA model\n",
        "  lda_model = LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics, passes=2)\n",
        "  return lda_model"
      ],
      "metadata": {
        "id": "xf3surfWXH-q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST id2word and corpus creation\n",
        "dictionary, corpus = create_corpus(processed_data)"
      ],
      "metadata": {
        "id": "D9m6sET1jaji"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dictionary[0]) # {0: \"bandicoot\", 1: \"crash\"}\n",
        "print(processed_data['Control'][0])\n",
        "print(corpus[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VabTeBTCrmLn",
        "outputId": "9b001294-b20d-4f92-898b-6f16d15e0b63"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bandicoot\n",
            "['man', 'love', 'bandicoot', 'crash']\n",
            "[(0, 1), (1, 1), (2, 1), (3, 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST lda model\n",
        "lda_model = train_lda(dictionary, corpus)"
      ],
      "metadata": {
        "id": "EhjEUjj-rX0O"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview topics\n",
        "topics = lda_model.print_topics(num_topics=200, num_words=10)\n",
        "for ith_topic, topic in enumerate(topics):\n",
        "    print(ith_topic, \"Topics:\", topic[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b9c5cf-aefa-4363-d06b-51750e53d8f2",
        "id": "2TnhtkNnkU6S"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Topics: 0.039*\"trial\" + 0.025*\"pot\" + 0.023*\"pace\" + 0.022*\"saved\" + 0.019*\"tip\" + 0.018*\"humiliating\" + 0.015*\"destructive\" + 0.014*\"ignorant\" + 0.013*\"solo\" + 0.012*\"criticism\"\n",
            "1 Topics: 0.026*\"injury\" + 0.024*\"liver\" + 0.023*\"finger\" + 0.021*\"migraines\" + 0.021*\"scan\" + 0.020*\"solutions\" + 0.017*\"ct\" + 0.010*\"sweats\" + 0.010*\"ah\" + 0.009*\"wired\"\n",
            "2 Topics: 0.076*\"social\" + 0.041*\"anxiety\" + 0.017*\"awkward\" + 0.016*\"feel\" + 0.015*\"being\" + 0.012*\"shy\" + 0.011*\"interactions\" + 0.011*\"interaction\" + 0.010*\"conversations\" + 0.008*\"judging\"\n",
            "3 Topics: 0.095*\"wanna\" + 0.030*\"feel\" + 0.019*\"die\" + 0.014*\"pointless\" + 0.012*\"everyone\" + 0.011*\"everything\" + 0.011*\"alone\" + 0.010*\"someone\" + 0.010*\"myself\" + 0.009*\"belong\"\n",
            "4 Topics: 0.023*\"partly\" + 0.021*\"inevitable\" + 0.016*\"agency\" + 0.012*\"landlord\" + 0.012*\"keto\" + 0.009*\"weaker\" + 0.009*\"bitterness\" + 0.008*\"observe\" + 0.007*\"annual\" + 0.007*\"bless\"\n",
            "5 Topics: 0.015*\"autism\" + 0.015*\"their\" + 0.014*\"science\" + 0.012*\"knowledge\" + 0.012*\"autistic\" + 0.012*\"addiction\" + 0.012*\"program\" + 0.012*\"skills\" + 0.011*\"bike\" + 0.010*\"data\"\n",
            "6 Topics: 0.047*\"crisis\" + 0.028*\"concert\" + 0.021*\"existential\" + 0.020*\"society\" + 0.017*\"survival\" + 0.014*\"virtually\" + 0.013*\"twin\" + 0.012*\"unexpected\" + 0.012*\"covering\" + 0.012*\"belongs\"\n",
            "7 Topics: 0.140*\"pain\" + 0.021*\"doctor\" + 0.015*\"chronic\" + 0.011*\"side\" + 0.009*\"muscle\" + 0.009*\"doctors\" + 0.008*\"after\" + 0.008*\"left\" + 0.008*\"worse\" + 0.008*\"severe\"\n",
            "8 Topics: 0.016*\"animals\" + 0.014*\"acne\" + 0.011*\"animal\" + 0.010*\"paranoid\" + 0.009*\"mind\" + 0.008*\"psychotic\" + 0.007*\"towel\" + 0.007*\"dopamine\" + 0.006*\"surreal\" + 0.006*\"sheets\"\n",
            "9 Topics: 0.024*\"feeling\" + 0.017*\"anxiety\" + 0.016*\"breath\" + 0.015*\"head\" + 0.015*\"feels\" + 0.014*\"air\" + 0.014*\"body\" + 0.012*\"feel\" + 0.012*\"breathing\" + 0.008*\"weird\"\n",
            "10 Topics: 0.083*\"anyone\" + 0.076*\"does\" + 0.058*\"else\" + 0.055*\"heart\" + 0.028*\"rate\" + 0.027*\"anxiety\" + 0.011*\"feel\" + 0.011*\"nervous\" + 0.010*\"uncomfortable\" + 0.010*\"flight\"\n",
            "11 Topics: 0.040*\"money\" + 0.023*\"pay\" + 0.019*\"$\" + 0.019*\"kids\" + 0.013*\"years\" + 0.011*\"debt\" + 0.010*\"house\" + 0.010*\"afford\" + 0.009*\"bills\" + 0.009*\"job\"\n",
            "12 Topics: 0.091*\"sa\" + 0.042*\"oil\" + 0.029*\"cbd\" + 0.015*\"rejected\" + 0.015*\"consequences\" + 0.011*\"inferior\" + 0.008*\"switching\" + 0.008*\"using\" + 0.008*\"slave\" + 0.008*\"disappointing\"\n",
            "13 Topics: 0.104*\"sad\" + 0.038*\"Ã©\" + 0.033*\"fianc\" + 0.021*\"songs\" + 0.016*\"members\" + 0.014*\"army\" + 0.014*\"victory\" + 0.013*\"assault\" + 0.013*\"7th\" + 0.012*\"aversion\"\n",
            "14 Topics: 0.040*\"girl\" + 0.028*\"crush\" + 0.025*\"dream\" + 0.024*\"virgin\" + 0.023*\"ugh\" + 0.018*\"never\" + 0.016*\"kissed\" + 0.013*\"bye\" + 0.012*\"girls\" + 0.012*\"someone\"\n",
            "15 Topics: 0.020*\"culture\" + 0.017*\"america\" + 0.016*\"smiling\" + 0.013*\"twitter\" + 0.012*\"american\" + 0.012*\"among\" + 0.010*\"moon\" + 0.009*\"chin\" + 0.009*\"country\" + 0.008*\"forehead\"\n",
            "16 Topics: 0.015*\"job\" + 0.012*\"goals\" + 0.009*\"life\" + 0.008*\"achieve\" + 0.007*\"peoples\" + 0.007*\"being\" + 0.006*\"raise\" + 0.006*\"make\" + 0.006*\"defeated\" + 0.006*\"why\"\n",
            "17 Topics: 0.087*\"â€”\" + 0.030*\"whats\" + 0.029*\"bpd\" + 0.023*\"feed\" + 0.021*\"stages\" + 0.020*\"drops\" + 0.019*\"mattress\" + 0.016*\"negatively\" + 0.016*\"candy\" + 0.016*\"french\"\n",
            "18 Topics: 0.044*\"system\" + 0.014*\"mood\" + 0.013*\"mindset\" + 0.012*\"breakdown\" + 0.010*\"anxiety\" + 0.010*\"swings\" + 0.008*\"resulting\" + 0.008*\"guidance\" + 0.006*\"help\" + 0.006*\"juice\"\n",
            "19 Topics: 0.019*\"friend\" + 0.018*\"told\" + 0.013*\"she's\" + 0.012*\"boss\" + 0.011*\"ask\" + 0.009*\"i'll\" + 0.009*\"said\" + 0.008*\"call\" + 0.008*\"help\" + 0.008*\"asked\"\n",
            "20 Topics: 0.014*\"mistakes\" + 0.013*\"screw\" + 0.012*\"freedom\" + 0.011*\"prison\" + 0.011*\"learn\" + 0.007*\"can't\" + 0.006*\"god\" + 0.006*\"were\" + 0.006*\"say\" + 0.006*\"something\"\n",
            "21 Topics: 0.028*\"clean\" + 0.016*\"again\" + 0.014*\"relapse\" + 0.014*\"days\" + 0.011*\"months\" + 0.011*\"relapsed\" + 0.009*\"years\" + 0.009*\"urges\" + 0.009*\"last\" + 0.008*\"night\"\n",
            "22 Topics: 0.085*\"x\" + 0.078*\"w\" + 0.060*\"n\" + 0.023*\"upped\" + 0.021*\"subtle\" + 0.016*\"`\" + 0.013*\"reconnected\" + 0.013*\"pops\" + 0.010*\"draw\" + 0.009*\"selective\"\n",
            "23 Topics: 0.021*\"feel\" + 0.018*\"life\" + 0.016*\"friends\" + 0.010*\"myself\" + 0.009*\"being\" + 0.008*\"make\" + 0.008*\"things\" + 0.007*\"always\" + 0.007*\"alone\" + 0.007*\"their\"\n",
            "24 Topics: 0.064*\"letter\" + 0.061*\"regret\" + 0.027*\"strategies\" + 0.025*\"overreacting\" + 0.020*\"cheers\" + 0.018*\"degrees\" + 0.017*\"masters\" + 0.015*\"prevention\" + 0.014*\"stepped\" + 0.014*\"cycles\"\n",
            "25 Topics: 0.029*\"feel\" + 0.021*\"angry\" + 0.014*\"myself\" + 0.013*\"anger\" + 0.012*\"feeling\" + 0.010*\"something\" + 0.009*\"being\" + 0.009*\"things\" + 0.009*\"can't\" + 0.009*\"cry\"\n",
            "26 Topics: 0.025*\"feel\" + 0.024*\"night\" + 0.021*\"feeling\" + 0.019*\"can't\" + 0.017*\"down\" + 0.013*\"bed\" + 0.012*\"nights\" + 0.011*\"calm\" + 0.011*\"last\" + 0.011*\"going\"\n",
            "27 Topics: 0.021*\"feel\" + 0.019*\"anxiety\" + 0.013*\"ms\" + 0.010*\"danger\" + 0.010*\"bothering\" + 0.010*\"detached\" + 0.009*\"function\" + 0.008*\"discomfort\" + 0.008*\"20mg\" + 0.008*\"anyone\"\n",
            "28 Topics: 0.068*\"...\" + 0.023*\"feel\" + 0.020*\"why\" + 0.009*\"here\" + 0.008*\"someone\" + 0.007*\"cuz\" + 0.007*\"photos\" + 0.007*\"always\" + 0.007*\"could\" + 0.007*\"delete\"\n",
            "29 Topics: 0.032*\"anxiety\" + 0.020*\"medication\" + 0.018*\"therapy\" + 0.013*\"disorder\" + 0.013*\"ocd\" + 0.012*\"help\" + 0.010*\"diagnosed\" + 0.010*\"therapist\" + 0.010*\"psychiatrist\" + 0.009*\"meds\"\n",
            "30 Topics: 0.098*\"school\" + 0.048*\"year\" + 0.043*\"college\" + 0.030*\"high\" + 0.019*\"friends\" + 0.015*\"classes\" + 0.010*\"semester\" + 0.008*\"since\" + 0.008*\"going\" + 0.008*\"student\"\n",
            "31 Topics: 0.019*\"disabled\" + 0.017*\"neighbors\" + 0.012*\"capacity\" + 0.010*\"reward\" + 0.010*\"muster\" + 0.010*\"decades\" + 0.010*\"recommendation\" + 0.007*\"adopted\" + 0.007*\"ironic\" + 0.007*\"obsess\"\n",
            "32 Topics: 0.055*\"kid\" + 0.018*\"else's\" + 0.016*\"parent\" + 0.016*\"teen\" + 0.016*\"faith\" + 0.013*\"soul\" + 0.012*\"destroy\" + 0.010*\"mechanisms\" + 0.008*\"someone\" + 0.008*\"childish\"\n",
            "33 Topics: 0.036*\"university\" + 0.015*\"studying\" + 0.012*\"study\" + 0.011*\"antidepressant\" + 0.011*\"anxiety\" + 0.010*\"obsessing\" + 0.010*\"take\" + 0.007*\"help\" + 0.006*\"which\" + 0.006*\"feel\"\n",
            "34 Topics: 0.028*\"fire\" + 0.025*\"cbt\" + 0.019*\"recovery\" + 0.018*\"seat\" + 0.015*\"cymbalta\" + 0.015*\"cognitive\" + 0.014*\"effective\" + 0.014*\"+\" + 0.014*\"presentations\" + 0.011*\"process\"\n",
            "35 Topics: 0.088*\"smile\" + 0.018*\"expression\" + 0.018*\"face\" + 0.017*\"fuckin\" + 0.014*\"facial\" + 0.013*\"aged\" + 0.012*\"smiled\" + 0.011*\"muscular\" + 0.010*\"fuzzy\" + 0.010*\"look\"\n",
            "36 Topics: 0.143*\"self\" + 0.069*\"harm\" + 0.027*\"title\" + 0.020*\"says\" + 0.019*\"anyone\" + 0.015*\"upset\" + 0.011*\"esteem\" + 0.011*\"message\" + 0.011*\"does\" + 0.008*\"feel\"\n",
            "37 Topics: 0.028*\"makeup\" + 0.020*\"asian\" + 0.019*\"fas\" + 0.019*\"ghosted\" + 0.016*\"normie\" + 0.014*\"prom\" + 0.011*\"infidelity\" + 0.011*\"25\" + 0.011*\"restroom\" + 0.010*\"figuring\"\n",
            "38 Topics: 0.034*\"sex\" + 0.010*\"our\" + 0.009*\"after\" + 0.007*\"off\" + 0.007*\"wife\" + 0.006*\"never\" + 0.006*\"maybe\" + 0.006*\"oral\" + 0.006*\"said\" + 0.006*\"first\"\n",
            "39 Topics: 0.047*\"hate\" + 0.038*\"myself\" + 0.028*\"feel\" + 0.013*\"life\" + 0.013*\"can't\" + 0.008*\"world\" + 0.008*\"everyone\" + 0.007*\"being\" + 0.007*\"make\" + 0.007*\"better\"\n",
            "40 Topics: 0.057*\"illness\" + 0.031*\"mental\" + 0.021*\"harming\" + 0.020*\"depression\" + 0.015*\"years\" + 0.013*\"14\" + 0.013*\"myself\" + 0.012*\"sadness\" + 0.012*\"self\" + 0.010*\"accomplish\"\n",
            "41 Topics: 0.012*\"yay\" + 0.012*\"can't\" + 0.011*\"feel\" + 0.010*\"hole\" + 0.010*\"rent\" + 0.009*\"employment\" + 0.009*\"spinning\" + 0.009*\"section\" + 0.008*\"take\" + 0.008*\"wellbutrin\"\n",
            "42 Topics: 0.033*\"anxiety\" + 0.014*\"depression\" + 0.012*\"which\" + 0.011*\"meds\" + 0.010*\"taking\" + 0.009*\"brain\" + 0.009*\"symptoms\" + 0.008*\"feel\" + 0.008*\"effects\" + 0.008*\"years\"\n",
            "43 Topics: 0.056*\"deleted\" + 0.022*\"reddit\" + 0.018*\"route\" + 0.016*\"responding\" + 0.014*\"bet\" + 0.012*\"sea\" + 0.012*\"medicines\" + 0.012*\"sub\" + 0.010*\"location\" + 0.010*\"Â£\"\n",
            "44 Topics: 0.015*\"selfish\" + 0.014*\"brain\" + 0.013*\"children\" + 0.012*\"suffering\" + 0.011*\"life\" + 0.011*\"things\" + 0.010*\"way\" + 0.009*\"can't\" + 0.009*\"person\" + 0.009*\"solution\"\n",
            "45 Topics: 0.089*\"music\" + 0.035*\"listen\" + 0.034*\"stories\" + 0.033*\"listening\" + 0.020*\"dae\" + 0.015*\"headphones\" + 0.012*\"war\" + 0.012*\"vs\" + 0.010*\"read\" + 0.008*\"knee\"\n",
            "46 Topics: 0.081*\"health\" + 0.062*\"mental\" + 0.010*\"anxiety\" + 0.010*\"issues\" + 0.009*\"logical\" + 0.009*\"help\" + 0.009*\"button\" + 0.008*\"services\" + 0.007*\"life\" + 0.007*\"destroyed\"\n",
            "47 Topics: 0.022*\"girl\" + 0.016*\"said\" + 0.010*\"didn't\" + 0.009*\"asked\" + 0.009*\"she's\" + 0.008*\"talking\" + 0.007*\"got\" + 0.007*\"friend\" + 0.006*\"talk\" + 0.006*\"say\"\n",
            "48 Topics: 0.020*\"valium\" + 0.019*\"speech\" + 0.016*\"e\" + 0.014*\"tree\" + 0.012*\"stuttering\" + 0.012*\"de\" + 0.012*\"functional\" + 0.011*\"combo\" + 0.010*\"se\" + 0.010*\"lasts\"\n",
            "49 Topics: 0.071*\"watch\" + 0.044*\"tv\" + 0.041*\"watching\" + 0.040*\"movie\" + 0.034*\"movies\" + 0.018*\"favorite\" + 0.017*\"show\" + 0.016*\"annoyed\" + 0.013*\"books\" + 0.013*\"tone\"\n",
            "50 Topics: 0.059*\"o\" + 0.049*\"female\" + 0.040*\"y\" + 0.039*\"male\" + 0.037*\"fat\" + 0.026*\"thanksgiving\" + 0.025*\"sore\" + 0.022*\"activities\" + 0.018*\"occasional\" + 0.013*\"impact\"\n",
            "51 Topics: 0.037*\"teeth\" + 0.016*\"bleeding\" + 0.015*\"brush\" + 0.013*\"alright\" + 0.012*\"replied\" + 0.011*\"bones\" + 0.010*\"brushed\" + 0.010*\"commented\" + 0.009*\"press\" + 0.009*\"record\"\n",
            "52 Topics: 0.355*\"..\" + 0.038*\"anybody\" + 0.028*\"...\" + 0.017*\"scars\" + 0.010*\"arms\" + 0.009*\"heal\" + 0.008*\"else\" + 0.007*\"legs\" + 0.007*\"idk\" + 0.007*\"harmed\"\n",
            "53 Topics: 0.017*\"garbage\" + 0.016*\"depressing\" + 0.016*\"dropping\" + 0.016*\"isolation\" + 0.014*\"winter\" + 0.013*\"returning\" + 0.012*\"rich\" + 0.012*\"semester\" + 0.011*\"clothes\" + 0.010*\"rock\"\n",
            "54 Topics: 0.026*\"bus\" + 0.008*\"got\" + 0.007*\"snapped\" + 0.007*\"years\" + 0.006*\"way\" + 0.006*\"fly\" + 0.006*\"i'd\" + 0.006*\"go\" + 0.006*\"very\" + 0.005*\"did\"\n",
            "55 Topics: 0.081*\"failure\" + 0.035*\"news\" + 0.027*\"flu\" + 0.013*\"comforting\" + 0.011*\"catching\" + 0.009*\"immune\" + 0.009*\"feel\" + 0.008*\"bits\" + 0.008*\"sister's\" + 0.007*\"self-harming\"\n",
            "56 Topics: 0.042*\"eat\" + 0.023*\"drink\" + 0.021*\"day\" + 0.021*\"stomach\" + 0.019*\"eating\" + 0.016*\"drinking\" + 0.013*\"food\" + 0.012*\"feel\" + 0.010*\"after\" + 0.010*\"water\"\n",
            "57 Topics: 0.031*\"christmas\" + 0.017*\"leg\" + 0.017*\"pain\" + 0.009*\"relief\" + 0.007*\"disc\" + 0.007*\"mri\" + 0.007*\"injections\" + 0.006*\"pt\" + 0.006*\"lower\" + 0.005*\"beats\"\n",
            "58 Topics: 0.045*\"fear\" + 0.023*\"weird\" + 0.021*\"something\" + 0.013*\"feeling\" + 0.010*\"head\" + 0.009*\"into\" + 0.007*\"being\" + 0.007*\"feel\" + 0.006*\"see\" + 0.006*\"thing\"\n",
            "59 Topics: 0.020*\"card\" + 0.019*\"credit\" + 0.016*\"thru\" + 0.014*\"grandmother\" + 0.012*\"bouts\" + 0.012*\"fearing\" + 0.009*\"religious\" + 0.009*\"3-5\" + 0.007*\"hospital\" + 0.007*\"wether\"\n",
            "60 Topics: 0.180*\"â€™\" + 0.062*\"t\" + 0.048*\"m\" + 0.037*\"s\" + 0.024*\"don\" + 0.021*\"ve\" + 0.010*\"â€\" + 0.010*\"â€œ\" + 0.009*\"feel\" + 0.006*\"didn\"\n",
            "61 Topics: 0.031*\"%\" + 0.031*\"update\" + 0.028*\"friday\" + 0.028*\"monday\" + 0.024*\".....\" + 0.022*\"sunday\" + 0.022*\"saturday\" + 0.018*\"marriage\" + 0.017*\"wife\" + 0.016*\"week\"\n",
            "62 Topics: 0.026*\"ativan\" + 0.011*\"depend\" + 0.011*\"withdrawals\" + 0.009*\"ruled\" + 0.009*\"allergies\" + 0.009*\"accompanied\" + 0.009*\"intestinal\" + 0.009*\"tripping\" + 0.008*\"impacted\" + 0.008*\"inferiority\"\n",
            "63 Topics: 0.018*\"wheel\" + 0.018*\"bump\" + 0.015*\"celebrate\" + 0.015*\"embarrassment\" + 0.011*\"grateful\" + 0.010*\"acknowledge\" + 0.009*\"...\" + 0.009*\"nerves\" + 0.006*\"something\" + 0.006*\"spell\"\n",
            "64 Topics: 0.042*\"cancer\" + 0.020*\"googling\" + 0.016*\"abnormal\" + 0.015*\"clothing\" + 0.013*\"stigma\" + 0.011*\"pisses\" + 0.007*\"degenerative\" + 0.006*\"google\" + 0.006*\"literature\" + 0.006*\"recognized\"\n",
            "65 Topics: 0.064*\"headache\" + 0.028*\"tricks\" + 0.019*\"california\" + 0.017*\"vicious\" + 0.014*\"tumor\" + 0.013*\"brain\" + 0.013*\"hormone\" + 0.012*\"corner\" + 0.011*\"persistent\" + 0.011*\"distracted\"\n",
            "66 Topics: 0.092*\"lonely\" + 0.069*\"loneliness\" + 0.029*\"chat\" + 0.019*\"fake\" + 0.015*\"exams\" + 0.010*\"alone\" + 0.010*\"writing\" + 0.009*\"forum\" + 0.009*\"here\" + 0.008*\"help\"\n",
            "67 Topics: 0.078*\"gf\" + 0.045*\"holidays\" + 0.044*\"silence\" + 0.028*\"disorders\" + 0.027*\"wow\" + 0.024*\"action\" + 0.021*\"crap\" + 0.019*\"staff\" + 0.018*\"avoided\" + 0.014*\"cup\"\n",
            "68 Topics: 0.052*\"suicide\" + 0.027*\"hotline\" + 0.024*\"feel\" + 0.022*\"drowning\" + 0.014*\"belt\" + 0.012*\"call\" + 0.008*\"myself\" + 0.008*\"resort\" + 0.007*\"sickness\" + 0.007*\"committing\"\n",
            "69 Topics: 0.041*\"hands\" + 0.038*\"arm\" + 0.029*\"scars\" + 0.026*\"hand\" + 0.019*\"cuts\" + 0.017*\"fingers\" + 0.016*\"arms\" + 0.012*\"skin\" + 0.011*\"cover\" + 0.011*\"cut\"\n",
            "70 Topics: 0.030*\"bottle\" + 0.023*\"lymph\" + 0.023*\"chicken\" + 0.022*\"swollen\" + 0.015*\"nodes\" + 0.015*\"drastic\" + 0.012*\"semi\" + 0.012*\"unreal\" + 0.012*\"reducing\" + 0.012*\"fellow\"\n",
            "71 Topics: 0.024*\"conversation\" + 0.019*\"say\" + 0.014*\"talking\" + 0.012*\"feel\" + 0.012*\"speak\" + 0.011*\"talk\" + 0.009*\"questions\" + 0.009*\"speaking\" + 0.008*\"english\" + 0.007*\"someone\"\n",
            "72 Topics: 0.016*\"prepare\" + 0.014*\"yoga\" + 0.012*\"exercises\" + 0.011*\"classes\" + 0.010*\"professors\" + 0.010*\"work\" + 0.010*\"engineering\" + 0.008*\"magnesium\" + 0.008*\"paycheck\" + 0.008*\"inch\"\n",
            "73 Topics: 0.110*\"fucking\" + 0.071*\"fuck\" + 0.044*\"shit\" + 0.019*\"can't\" + 0.019*\"why\" + 0.018*\"nobody\" + 0.014*\"anymore\" + 0.011*\"life\" + 0.010*\"everything\" + 0.010*\"done\"\n",
            "74 Topics: 0.090*\"side\" + 0.056*\"effect\" + 0.049*\"effects\" + 0.029*\"birth\" + 0.028*\"waiting\" + 0.022*\"note\" + 0.020*\"control\" + 0.019*\"awhile\" + 0.019*\"schizophrenia\" + 0.017*\"depressive\"\n",
            "75 Topics: 0.305*\"removed\" + 0.063*\">\" + 0.023*\"roommate\" + 0.016*\"suicide\" + 0.014*\"help\" + 0.013*\"alone\" + 0.010*\"need\" + 0.009*\"anyone\" + 0.009*\"why\" + 0.009*\"...\"\n",
            "76 Topics: 0.069*\"neck\" + 0.017*\"pillow\" + 0.015*\"feet\" + 0.010*\"hypochondria\" + 0.010*\"head\" + 0.010*\"shirt\" + 0.010*\"chair\" + 0.009*\"tips\" + 0.009*\"tight\" + 0.008*\"moderate\"\n",
            "77 Topics: 0.016*\"creepy\" + 0.015*\"feel\" + 0.012*\"lie\" + 0.010*\"someone\" + 0.010*\"voice\" + 0.009*\"ashamed\" + 0.007*\"else\" + 0.007*\"you're\" + 0.007*\"...\" + 0.007*\"knock\"\n",
            "78 Topics: 0.034*\"games\" + 0.030*\"bf\" + 0.022*\"video\" + 0.019*\"playing\" + 0.014*\"play\" + 0.014*\"him\" + 0.009*\"apologize\" + 0.008*\"said\" + 0.007*\"tongue\" + 0.007*\"told\"\n",
            "79 Topics: 0.032*\"life\" + 0.023*\"myself\" + 0.013*\"can't\" + 0.011*\"kill\" + 0.010*\"die\" + 0.010*\"never\" + 0.010*\"feel\" + 0.010*\"end\" + 0.009*\"going\" + 0.008*\"anymore\"\n",
            "80 Topics: 0.044*\"job\" + 0.043*\"work\" + 0.022*\"anxiety\" + 0.016*\"can't\" + 0.015*\"go\" + 0.012*\"working\" + 0.012*\"home\" + 0.012*\"feel\" + 0.011*\"day\" + 0.011*\"going\"\n",
            "81 Topics: 0.068*\"â€¦\" + 0.057*\"avpd\" + 0.031*\"pets\" + 0.021*\"tbh\" + 0.020*\"ðŸ˜‚\" + 0.020*\"stem\" + 0.019*\"relaxing\" + 0.016*\"struggles\" + 0.016*\"annoy\" + 0.011*\"escitalopram\"\n",
            "82 Topics: 0.078*\"u\" + 0.048*\"account\" + 0.046*\"blah\" + 0.014*\"throwaway\" + 0.013*\"sweating\" + 0.011*\"tolerance\" + 0.011*\"nope\" + 0.010*\"cured\" + 0.008*\"persons\" + 0.008*\"complained\"\n",
            "83 Topics: 0.014*\"relationship\" + 0.011*\"him\" + 0.011*\"years\" + 0.010*\"our\" + 0.010*\"months\" + 0.009*\"after\" + 0.009*\"together\" + 0.009*\"didn't\" + 0.008*\"year\" + 0.008*\"feel\"\n",
            "84 Topics: 0.213*\"=\" + 0.020*\"st\" + 0.016*\"pattern\" + 0.012*\"emailed\" + 0.011*\"affairs\" + 0.010*\"sh\" + 0.007*\"+\" + 0.007*\"(:\" + 0.006*\"albeit\" + 0.006*\"pitiful\"\n",
            "85 Topics: 0.011*\"j\" + 0.010*\"lyrica\" + 0.010*\"hole\" + 0.007*\"intentionally\" + 0.007*\"exploring\" + 0.007*\"navigate\" + 0.006*\"beard\" + 0.006*\"most\" + 0.006*\"comedy\" + 0.006*\"purely\"\n",
            "86 Topics: 0.034*\"red\" + 0.033*\"ask\" + 0.028*\"friendship\" + 0.028*\"number\" + 0.017*\"allowed\" + 0.016*\"demons\" + 0.015*\"ring\" + 0.013*\"explanation\" + 0.012*\"bomb\" + 0.011*\"rash\"\n",
            "87 Topics: 0.062*\"cfs\" + 0.038*\"dr\" + 0.027*\"white\" + 0.017*\"cure\" + 0.017*\"disease\" + 0.013*\"sweat\" + 0.013*\"cell\" + 0.012*\"cough\" + 0.010*\"evidence\" + 0.010*\"dry\"\n",
            "88 Topics: 0.042*\"goodbye\" + 0.015*\"overdose\" + 0.014*\"blocked\" + 0.013*\"myself\" + 0.011*\"traffic\" + 0.010*\"step\" + 0.010*\"survived\" + 0.009*\"tearing\" + 0.008*\"hating\" + 0.007*\"again\"\n",
            "89 Topics: 0.110*\"job\" + 0.026*\"interview\" + 0.016*\"company\" + 0.014*\"position\" + 0.013*\"work\" + 0.013*\"quit\" + 0.010*\"jobs\" + 0.010*\"fired\" + 0.009*\"offer\" + 0.009*\"got\"\n",
            "90 Topics: 0.094*\"â€¢\" + 0.036*\"stresses\" + 0.030*\"noone\" + 0.027*\"roll\" + 0.019*\"21\" + 0.017*\"frank\" + 0.013*\"tank\" + 0.012*\"troubled\" + 0.011*\"wallet\" + 0.011*\"2/3\"\n",
            "91 Topics: 0.045*\"food\" + 0.023*\"eat\" + 0.018*\"foods\" + 0.009*\"lunch\" + 0.008*\"eating\" + 0.007*\"which\" + 0.007*\"breakfast\" + 0.007*\"also\" + 0.007*\"something\" + 0.006*\"cook\"\n",
            "92 Topics: 0.078*\"fa\" + 0.017*\"picture\" + 0.015*\"g\" + 0.015*\"alot\" + 0.015*\"tinder\" + 0.012*\"dating\" + 0.009*\"here\" + 0.009*\"looking\" + 0.009*\"guys\" + 0.009*\"fiance\"\n",
            "93 Topics: 0.023*\"health\" + 0.015*\"appointment\" + 0.014*\"fatigue\" + 0.010*\"doctor\" + 0.010*\"anxiety\" + 0.010*\"hot\" + 0.009*\"mental\" + 0.009*\"nurse\" + 0.008*\"appointments\" + 0.007*\"psychiatrist\"\n",
            "94 Topics: 0.159*\"him\" + 0.040*\"he's\" + 0.011*\"doesn't\" + 0.008*\"himself\" + 0.008*\"guy\" + 0.008*\"friend\" + 0.008*\"love\" + 0.007*\"boyfriend\" + 0.007*\"feel\" + 0.006*\"being\"\n",
            "95 Topics: 0.050*\"study\" + 0.044*\"ugly\" + 0.039*\"song\" + 0.036*\"average\" + 0.022*\"link\" + 0.019*\"research\" + 0.018*\"military\" + 0.016*\"below\" + 0.016*\"height\" + 0.014*\"psychology\"\n",
            "96 Topics: 0.046*\"drugs\" + 0.029*\"gp\" + 0.028*\"uk\" + 0.024*\"patients\" + 0.023*\"potential\" + 0.021*\"mg\" + 0.020*\"picking\" + 0.017*\"beer\" + 0.016*\"toilet\" + 0.016*\"extended\"\n",
            "97 Topics: 0.124*\"hair\" + 0.064*\"harder\" + 0.031*\"poor\" + 0.021*\"luck\" + 0.020*\"worker\" + 0.015*\"poverty\" + 0.012*\"pressing\" + 0.011*\"technique\" + 0.010*\"education\" + 0.010*\"karma\"\n",
            "98 Topics: 0.027*\"gained\" + 0.017*\"overweight\" + 0.014*\"counting\" + 0.012*\"skinny\" + 0.010*\"obese\" + 0.010*\"fmt\" + 0.009*\"weight\" + 0.009*\"shape\" + 0.008*\"pregnancy\" + 0.008*\"pounds\"\n",
            "99 Topics: 0.021*\"life\" + 0.009*\"feel\" + 0.009*\"myself\" + 0.007*\"things\" + 0.005*\"always\" + 0.005*\"very\" + 0.005*\"control\" + 0.005*\"point\" + 0.005*\"years\" + 0.005*\"where\"\n",
            "100 Topics: 0.055*\"tinnitus\" + 0.040*\"ear\" + 0.036*\"hearing\" + 0.033*\"loud\" + 0.028*\"hear\" + 0.025*\"noise\" + 0.025*\"sound\" + 0.019*\"ears\" + 0.017*\"ringing\" + 0.012*\"sounds\"\n",
            "101 Topics: 0.033*\"grade\" + 0.021*\"life\" + 0.013*\"exist\" + 0.010*\"feel\" + 0.010*\"purpose\" + 0.009*\"friends\" + 0.009*\"myself\" + 0.009*\"why\" + 0.007*\"care\" + 0.007*\"overthinking\"\n",
            "102 Topics: 0.036*\"uni\" + 0.028*\"country\" + 0.026*\"city\" + 0.023*\"math\" + 0.015*\"film\" + 0.014*\"bs\" + 0.010*\"enjoyment\" + 0.010*\"abroad\" + 0.008*\"study\" + 0.008*\"aka\"\n",
            "103 Topics: 0.031*\"stool\" + 0.025*\"k\" + 0.024*\"gut\" + 0.020*\"wise\" + 0.018*\"brown\" + 0.018*\"heroin\" + 0.015*\"mucus\" + 0.015*\"sibo\" + 0.014*\"uber\" + 0.014*\"ordered\"\n",
            "104 Topics: 0.021*\"sex\" + 0.019*\"partner\" + 0.013*\"years\" + 0.011*\"love\" + 0.010*\"db\" + 0.009*\"never\" + 0.009*\"feel\" + 0.009*\"relationship\" + 0.009*\"him\" + 0.008*\"ll\"\n",
            "105 Topics: 0.127*\"tomorrow\" + 0.117*\"tonight\" + 0.018*\"going\" + 0.017*\"painless\" + 0.012*\"halloween\" + 0.010*\"officially\" + 0.010*\"lmao\" + 0.009*\"camp\" + 0.008*\"way\" + 0.008*\"go\"\n",
            "106 Topics: 0.033*\"noises\" + 0.026*\"punch\" + 0.023*\"iv\" + 0.019*\"rn\" + 0.015*\"counter\" + 0.014*\"explains\" + 0.012*\"charge\" + 0.011*\"imagination\" + 0.011*\"forearm\" + 0.010*\"bang\"\n",
            "107 Topics: 0.042*\"hug\" + 0.022*\"invisible\" + 0.014*\"shyness\" + 0.013*\"singing\" + 0.012*\"feel\" + 0.011*\"sing\" + 0.009*\"froze\" + 0.009*\"bullies\" + 0.008*\"shrink\" + 0.007*\"hall\"\n",
            "108 Topics: 0.021*\"promised\" + 0.016*\"bubble\" + 0.016*\"motivated\" + 0.014*\"denying\" + 0.011*\"status\" + 0.011*\"15mg\" + 0.010*\"30mg\" + 0.009*\"hunt\" + 0.008*\"consistent\" + 0.008*\"pedestal\"\n",
            "109 Topics: 0.088*\"list\" + 0.036*\"blank\" + 0.035*\"page\" + 0.021*\"magic\" + 0.021*\"pharmacy\" + 0.018*\"bullet\" + 0.017*\"encouraging\" + 0.016*\"click\" + 0.014*\"filling\" + 0.013*\"laundry\"\n",
            "110 Topics: 0.307*\"'\" + 0.017*\"phone\" + 0.010*\"shoot\" + 0.007*\"say\" + 0.006*\"favor\" + 0.006*\"i'll\" + 0.006*\"never\" + 0.005*\"yo\" + 0.005*\"industry\" + 0.005*\"sales\"\n",
            "111 Topics: 0.078*\"sleep\" + 0.028*\"hours\" + 0.022*\"night\" + 0.019*\"wake\" + 0.018*\"bed\" + 0.016*\"day\" + 0.015*\"sleeping\" + 0.012*\"go\" + 0.008*\"before\" + 0.008*\"morning\"\n",
            "112 Topics: 0.021*\"our\" + 0.020*\"meditation\" + 0.020*\"natural\" + 0.012*\"healing\" + 0.010*\"help\" + 0.009*\"us\" + 0.008*\"guide\" + 0.007*\"awareness\" + 0.007*\"remedies\" + 0.007*\"anxiety\"\n",
            "113 Topics: 0.040*\"removed\" + 0.034*\"messages\" + 0.028*\"points\" + 0.028*\"gifts\" + 0.027*\"alternatives\" + 0.024*\"humor\" + 0.024*\"texts\" + 0.023*\"reassure\" + 0.022*\"engaging\" + 0.020*\"ain't\"\n",
            "114 Topics: 0.055*\"anxiety\" + 0.045*\"panic\" + 0.024*\"attack\" + 0.023*\"attacks\" + 0.014*\"feel\" + 0.012*\"having\" + 0.012*\"going\" + 0.008*\"bad\" + 0.008*\"started\" + 0.007*\"go\"\n",
            "115 Topics: 0.025*\"overcome\" + 0.015*\"anxiety\" + 0.010*\"tasks\" + 0.009*\"myself\" + 0.008*\"being\" + 0.008*\"work\" + 0.008*\"field\" + 0.008*\"security\" + 0.008*\"life\" + 0.007*\"feel\"\n",
            "116 Topics: 0.056*\"thoughts\" + 0.037*\"suicidal\" + 0.024*\"feel\" + 0.022*\"help\" + 0.015*\"depression\" + 0.009*\"these\" + 0.009*\"myself\" + 0.009*\"suicide\" + 0.008*\"therapist\" + 0.008*\"hospital\"\n",
            "117 Topics: 0.015*\"friend\" + 0.013*\"childhood\" + 0.013*\"dogs\" + 0.012*\"friend's\" + 0.011*\"committed\" + 0.011*\"psychiatric\" + 0.009*\"humans\" + 0.008*\"abused\" + 0.008*\"atleast\" + 0.008*\"react\"\n",
            "118 Topics: 0.042*\"hurts\" + 0.023*\"feel\" + 0.012*\"journal\" + 0.011*\"feedback\" + 0.011*\"things\" + 0.008*\"never\" + 0.008*\"wrong\" + 0.007*\"messing\" + 0.007*\"myself\" + 0.006*\"help\"\n",
            "119 Topics: 0.057*\"methods\" + 0.025*\"testing\" + 0.017*\"code\" + 0.017*\"valuable\" + 0.015*\"dump\" + 0.015*\"clients\" + 0.013*\"split\" + 0.010*\"chemicals\" + 0.009*\"owner\" + 0.008*\"continually\"\n",
            "120 Topics: 0.065*\"holiday\" + 0.022*\"finals\" + 0.021*\"compare\" + 0.019*\"bath\" + 0.015*\"season\" + 0.013*\"ish\" + 0.012*\"license\" + 0.011*\"dread\" + 0.011*\"shadow\" + 0.010*\"colleges\"\n",
            "121 Topics: 0.149*\"cut\" + 0.046*\"cutting\" + 0.018*\"feel\" + 0.017*\"topic\" + 0.017*\"cuts\" + 0.015*\"deep\" + 0.013*\"free\" + 0.012*\"reminder\" + 0.010*\"sub\" + 0.010*\"wrist\"\n",
            "122 Topics: 0.038*\"train\" + 0.037*\"trip\" + 0.033*\"travel\" + 0.011*\"bars\" + 0.011*\"go\" + 0.010*\"day\" + 0.010*\"pregnant\" + 0.008*\"trips\" + 0.008*\"hill\" + 0.007*\"occurs\"\n",
            "123 Topics: 0.081*\"class\" + 0.031*\"anxious\" + 0.016*\"feel\" + 0.015*\"computer\" + 0.010*\"front\" + 0.009*\"anxiety\" + 0.009*\"text\" + 0.008*\"talk\" + 0.008*\"something\" + 0.008*\"whenever\"\n",
            "124 Topics: 0.014*\"head\" + 0.013*\"nose\" + 0.013*\"nightmares\" + 0.012*\"eyes\" + 0.011*\"right\" + 0.008*\"nightmare\" + 0.008*\"sensation\" + 0.008*\"woke\" + 0.007*\"started\" + 0.007*\"jaw\"\n",
            "125 Topics: 0.045*\"er\" + 0.029*\"gender\" + 0.025*\"af\" + 0.024*\"star\" + 0.021*\"apple\" + 0.020*\"identity\" + 0.019*\"tremor\" + 0.017*\"device\" + 0.014*\"embrace\" + 0.013*\"et\"\n",
            "126 Topics: 0.036*\"group\" + 0.030*\"gay\" + 0.024*\"center\" + 0.022*\"table\" + 0.016*\"therapist\" + 0.013*\"guitar\" + 0.013*\"couples\" + 0.013*\"relieve\" + 0.012*\"walks\" + 0.009*\"groups\"\n",
            "127 Topics: 0.082*\"friends\" + 0.046*\"talk\" + 0.030*\"someone\" + 0.027*\"anyone\" + 0.017*\"make\" + 0.016*\"friend\" + 0.015*\"feel\" + 0.015*\"group\" + 0.013*\"need\" + 0.013*\"else\"\n",
            "128 Topics: 0.069*\"you're\" + 0.055*\"yourself\" + 0.023*\"https://www.youtube.com/watch\" + 0.023*\"anxiety\" + 0.022*\"us\" + 0.017*\"v\" + 0.016*\"community\" + 0.015*\"free\" + 0.013*\"our\" + 0.013*\"those\"\n",
            "129 Topics: 0.025*\"sibo\" + 0.023*\"):\" + 0.022*\"bacteria\" + 0.021*\"dose\" + 0.015*\"butter\" + 0.012*\"intestine\" + 0.011*\"usage\" + 0.011*\"excellent\" + 0.011*\"peanut\" + 0.010*\"skull\"\n",
            "130 Topics: 0.072*\"^\" + 0.046*\"Â´\" + 0.031*\"improved\" + 0.026*\"cannabis\" + 0.024*\"propranolol\" + 0.023*\"usa\" + 0.020*\"beta\" + 0.018*\"team\" + 0.014*\"laws\" + 0.012*\"peacefully\"\n",
            "131 Topics: 0.022*\"pizza\" + 0.013*\"knees\" + 0.011*\"pants\" + 0.010*\"lines\" + 0.009*\"artist\" + 0.008*\"fancy\" + 0.008*\"tummy\" + 0.007*\"blur\" + 0.007*\"pre\" + 0.006*\"cord\"\n",
            "132 Topics: 0.074*\"weight\" + 0.047*\"eating\" + 0.033*\"binge\" + 0.019*\"myself\" + 0.016*\"gain\" + 0.015*\"lose\" + 0.014*\"body\" + 0.013*\"feel\" + 0.012*\"food\" + 0.012*\"pounds\"\n",
            "133 Topics: 0.065*\"curious\" + 0.036*\"date\" + 0.030*\"texting\" + 0.023*\"plans\" + 0.022*\"dates\" + 0.021*\"challenge\" + 0.017*\"normies\" + 0.013*\"phrase\" + 0.013*\"seizure\" + 0.011*\"involves\"\n",
            "134 Topics: 0.035*\"relate\" + 0.028*\"loser\" + 0.025*\"online\" + 0.019*\"irl\" + 0.018*\"pity\" + 0.013*\"look\" + 0.013*\"dude\" + 0.013*\"anyone\" + 0.011*\"appearance\" + 0.011*\"virginity\"\n",
            "135 Topics: 0.040*\"asleep\" + 0.028*\"fall\" + 0.024*\"sleep\" + 0.018*\"awake\" + 0.017*\"falling\" + 0.015*\"feel\" + 0.014*\"night\" + 0.007*\"wake\" + 0.007*\"bed\" + 0.007*\"waking\"\n",
            "136 Topics: 0.034*\"thread\" + 0.023*\"medicine\" + 0.015*\"milk\" + 0.015*\"sugar\" + 0.012*\"discussion\" + 0.012*\"tempted\" + 0.011*\"frozen\" + 0.010*\"mix\" + 0.010*\"overnight\" + 0.009*\"week\"\n",
            "137 Topics: 0.191*\"please\" + 0.069*\"bc\" + 0.026*\"article\" + 0.022*\"help\" + 0.015*\"buddy\" + 0.014*\"hotel\" + 0.012*\"inpatient\" + 0.011*\"festival\" + 0.010*\"netflix\" + 0.008*\"floating\"\n",
            "138 Topics: 0.059*\"facebook\" + 0.054*\"media\" + 0.045*\"app\" + 0.018*\"social\" + 0.018*\"post\" + 0.016*\"instagram\" + 0.015*\"cars\" + 0.014*\"blog\" + 0.014*\"wondered\" + 0.011*\"commit\"\n",
            "139 Topics: 0.027*\"abuse\" + 0.024*\"notes\" + 0.020*\"police\" + 0.016*\"victim\" + 0.013*\"court\" + 0.013*\"violence\" + 0.013*\"assaulted\" + 0.010*\"crime\" + 0.009*\"denial\" + 0.008*\"kik\"\n",
            "140 Topics: 0.018*\"headaches\" + 0.017*\"eye\" + 0.015*\"ideas\" + 0.015*\"reassurance\" + 0.012*\"vision\" + 0.011*\"anxiety\" + 0.011*\"developing\" + 0.010*\"symptoms\" + 0.010*\"brain\" + 0.008*\"digestive\"\n",
            "141 Topics: 0.071*\"im\" + 0.039*\"dont\" + 0.029*\"its\" + 0.022*\"cant\" + 0.019*\"feel\" + 0.017*\"ive\" + 0.014*\"myself\" + 0.009*\"anymore\" + 0.009*\"...\" + 0.008*\"help\"\n",
            "142 Topics: 0.109*\"knife\" + 0.091*\"kitchen\" + 0.034*\"h\" + 0.033*\"sharp\" + 0.028*\"shooting\" + 0.026*\"server\" + 0.023*\"vulnerable\" + 0.021*\"stab\" + 0.020*\"blowing\" + 0.019*\"keys\"\n",
            "143 Topics: 0.201*\"tired\" + 0.022*\"feel\" + 0.019*\"being\" + 0.012*\"too\" + 0.010*\"anymore\" + 0.007*\"always\" + 0.007*\"life\" + 0.007*\"sports\" + 0.007*\"exhausted\" + 0.006*\"alone\"\n",
            "144 Topics: 0.050*\"treat\" + 0.048*\"name\" + 0.043*\"dance\" + 0.021*\"parties\" + 0.020*\"l\" + 0.016*\"dancing\" + 0.014*\"portion\" + 0.014*\"terror\" + 0.013*\"ultimate\" + 0.012*\"no-one\"\n",
            "145 Topics: 0.017*\"never\" + 0.012*\"things\" + 0.012*\"life\" + 0.011*\"feel\" + 0.010*\"always\" + 0.009*\"still\" + 0.009*\"myself\" + 0.009*\"talk\" + 0.008*\"friends\" + 0.008*\"better\"\n",
            "146 Topics: 0.015*\"ibs-d\" + 0.008*\"episode\" + 0.007*\"years\" + 0.007*\"peaceful\" + 0.006*\"feel\" + 0.006*\"bad\" + 0.006*\"which\" + 0.006*\"constipated\" + 0.006*\"episodes\" + 0.006*\"seems\"\n",
            "147 Topics: 0.040*\"ibs\" + 0.032*\"blood\" + 0.031*\"symptoms\" + 0.021*\"test\" + 0.016*\"tests\" + 0.011*\"normal\" + 0.010*\"also\" + 0.010*\"which\" + 0.009*\"after\" + 0.008*\"anyone\"\n",
            "148 Topics: 0.050*\"sex\" + 0.015*\"our\" + 0.010*\"relationship\" + 0.009*\"feel\" + 0.008*\"years\" + 0.008*\"wife\" + 0.007*\"times\" + 0.007*\"things\" + 0.006*\"together\" + 0.006*\"very\"\n",
            "149 Topics: 0.044*\"insomnia\" + 0.036*\"sleep\" + 0.017*\"reduce\" + 0.014*\"snoring\" + 0.013*\"night\" + 0.010*\"restless\" + 0.010*\"ambien\" + 0.008*\"stress\" + 0.008*\"deprivation\" + 0.007*\"factors\"\n",
            "150 Topics: 0.042*\"method\" + 0.026*\"sites\" + 0.024*\"blow\" + 0.022*\"site\" + 0.022*\"brains\" + 0.022*\"emails\" + 0.020*\"extra\" + 0.018*\"professionals\" + 0.013*\"meetup\" + 0.012*\"recommend\"\n",
            "151 Topics: 0.032*\"2018\" + 0.026*\"flat\" + 0.024*\"bag\" + 0.014*\"service\" + 0.012*\"pit\" + 0.010*\"acute\" + 0.008*\"4th\" + 0.008*\"......\" + 0.007*\"swim\" + 0.007*\"housing\"\n",
            "152 Topics: 0.009*\"love\" + 0.009*\"romantic\" + 0.008*\"cross\" + 0.008*\"each\" + 0.007*\"opening\" + 0.006*\"fantasy\" + 0.006*\"partner\" + 0.006*\"i'd\" + 0.006*\"our\" + 0.006*\"ourselves\"\n",
            "153 Topics: 0.073*\"women\" + 0.036*\"girls\" + 0.028*\"attractive\" + 0.024*\"men\" + 0.022*\"looks\" + 0.017*\"guy\" + 0.017*\"guys\" + 0.011*\"ugly\" + 0.011*\"dating\" + 0.010*\"woman\"\n",
            "154 Topics: 0.021*\"bridge\" + 0.018*\"creep\" + 0.013*\"off\" + 0.010*\"laptop\" + 0.009*\"jump\" + 0.009*\"mall\" + 0.007*\"jumping\" + 0.007*\"germany\" + 0.007*\"got\" + 0.006*\"programming\"\n",
            "155 Topics: 0.024*\"internship\" + 0.019*\"sufferers\" + 0.013*\"borderline\" + 0.013*\"anxiety\" + 0.011*\"north\" + 0.010*\"london\" + 0.010*\"unlikely\" + 0.010*\"subs\" + 0.009*\"procedures\" + 0.009*\"possibilities\"\n",
            "156 Topics: 0.042*\"2017\" + 0.022*\"earth\" + 0.021*\":D\" + 0.019*\"diseases\" + 0.017*\"essential\" + 0.016*\"burst\" + 0.015*\"linked\" + 0.015*\"theory\" + 0.014*\"disease\" + 0.014*\"asthma\"\n",
            "157 Topics: 0.043*\"anxiety\" + 0.036*\"social\" + 0.020*\"help\" + 0.013*\"feel\" + 0.012*\"situations\" + 0.010*\"make\" + 0.009*\"contact\" + 0.008*\"need\" + 0.008*\"someone\" + 0.007*\"myself\"\n",
            "158 Topics: 0.026*\"party\" + 0.011*\"were\" + 0.010*\"went\" + 0.009*\"friends\" + 0.009*\"didn't\" + 0.009*\"myself\" + 0.008*\"being\" + 0.008*\"made\" + 0.007*\"social\" + 0.006*\"everyone\"\n",
            "159 Topics: 0.046*\"teacher\" + 0.021*\"block\" + 0.019*\"memories\" + 0.018*\"pieces\" + 0.014*\"seek\" + 0.012*\"outlet\" + 0.012*\"hook\" + 0.010*\"realise\" + 0.010*\"marks\" + 0.009*\"psychological\"\n",
            "160 Topics: 0.018*\"can't\" + 0.014*\"parents\" + 0.013*\"myself\" + 0.012*\"feel\" + 0.010*\"help\" + 0.010*\"family\" + 0.009*\"life\" + 0.008*\"going\" + 0.008*\"years\" + 0.007*\"being\"\n",
            "161 Topics: 0.122*\"dog\" + 0.038*\"â€“\" + 0.019*\"flying\" + 0.019*\"weather\" + 0.016*\"warm\" + 0.014*\"phobia\" + 0.013*\"stitches\" + 0.012*\"healed\" + 0.012*\"puppy\" + 0.011*\"temperature\"\n",
            "162 Topics: 0.049*\"driving\" + 0.031*\"drive\" + 0.027*\"gym\" + 0.019*\"car\" + 0.017*\"game\" + 0.015*\"games\" + 0.012*\"play\" + 0.011*\"go\" + 0.011*\"can't\" + 0.011*\"myself\"\n",
            "163 Topics: 0.029*\"recommendations\" + 0.023*\"aches\" + 0.022*\"steps\" + 0.022*\"youtube\" + 0.015*\"choking\" + 0.013*\"shift\" + 0.012*\"ocean\" + 0.011*\"hiking\" + 0.011*\"co-workers\" + 0.011*\"spanish\"\n",
            "164 Topics: 0.080*\"car\" + 0.042*\"accident\" + 0.039*\"insurance\" + 0.015*\"money\" + 0.013*\"savings\" + 0.013*\"200\" + 0.011*\"crash\" + 0.009*\"pocket\" + 0.009*\"wind\" + 0.009*\"market\"\n",
            "165 Topics: 0.105*\"nbsp\" + 0.101*\"~\" + 0.007*\"remaining\" + 0.006*\"hangs\" + 0.006*\"reaches\" + 0.006*\"psycho\" + 0.005*\"flair\" + 0.005*\"killers\" + 0.005*\"progressed\" + 0.004*\"used\"\n",
            "166 Topics: 0.036*\"zoloft\" + 0.018*\"burning\" + 0.015*\"stress\" + 0.015*\"numb\" + 0.015*\"ptsd\" + 0.014*\"anxiety\" + 0.013*\"breathing\" + 0.012*\"trigger\" + 0.012*\"started\" + 0.012*\"cutting\"\n",
            "167 Topics: 0.100*\"cat\" + 0.028*\"cats\" + 0.024*\"swallow\" + 0.021*\"pet\" + 0.015*\"clonazepam\" + 0.012*\"feel\" + 0.011*\"cardiologist\" + 0.010*\"kitten\" + 0.009*\"parked\" + 0.009*\"hat\"\n",
            "168 Topics: 0.034*\"guilty\" + 0.033*\"guilt\" + 0.031*\"somebody\" + 0.024*\"feel\" + 0.017*\"addicted\" + 0.012*\"spiral\" + 0.009*\"facility\" + 0.009*\"downward\" + 0.008*\"matter\" + 0.008*\"myself\"\n",
            "169 Topics: 0.022*\"10mg\" + 0.021*\"limit\" + 0.020*\"round\" + 0.017*\"speed\" + 0.013*\"lock\" + 0.012*\"paxil\" + 0.011*\"safety\" + 0.009*\"blocks\" + 0.009*\"rape\" + 0.008*\"brave\"\n",
            "170 Topics: 0.223*\"r\" + 0.081*\"tl\" + 0.058*\";D\" + 0.055*\"subreddit\" + 0.011*\"post\" + 0.010*\"faster\" + 0.009*\"assholes\" + 0.008*\"anger\" + 0.007*\"coupled\" + 0.007*\"hilarious\"\n",
            "171 Topics: 0.042*\"ice\" + 0.027*\"cream\" + 0.015*\"â€™\" + 0.012*\"starving\" + 0.009*\"regretting\" + 0.008*\"lease\" + 0.008*\"myself\" + 0.008*\"shocking\" + 0.008*\"stoned\" + 0.008*\"concussion\"\n",
            "172 Topics: 0.011*\"feel\" + 0.011*\"broken\" + 0.009*\"helpless\" + 0.008*\"myself\" + 0.006*\"help\" + 0.006*\"didn't\" + 0.006*\"religion\" + 0.006*\"nothing\" + 0.006*\"could\" + 0.005*\"go\"\n",
            "173 Topics: 0.047*\"treatment\" + 0.042*\"medical\" + 0.028*\"chronic\" + 0.020*\"condition\" + 0.019*\"#\" + 0.019*\"syndrome\" + 0.013*\"treatments\" + 0.012*\"conditions\" + 0.012*\"disease\" + 0.011*\"d\"\n",
            "174 Topics: 0.033*\"disorder\" + 0.021*\"eating\" + 0.020*\"training\" + 0.019*\"videos\" + 0.017*\"gift\" + 0.012*\"successful\" + 0.012*\"path\" + 0.011*\"seeking\" + 0.011*\"threat\" + 0.011*\"lifetime\"\n",
            "175 Topics: 0.056*\"project\" + 0.036*\"choose\" + 0.031*\"choice\" + 0.028*\"trans\" + 0.026*\"y'all\" + 0.022*\"transition\" + 0.018*\"transgender\" + 0.016*\"deadline\" + 0.015*\"year's\" + 0.012*\"new\"\n",
            "176 Topics: 0.070*\"die\" + 0.024*\"email\" + 0.022*\"intrusive\" + 0.022*\"thoughts\" + 0.020*\"convince\" + 0.016*\"prevent\" + 0.016*\"deserve\" + 0.014*\"something\" + 0.014*\"weekend\" + 0.010*\"doing\"\n",
            "177 Topics: 0.084*\"birthday\" + 0.058*\"weed\" + 0.048*\"smoking\" + 0.042*\"smoke\" + 0.020*\"door\" + 0.013*\"doors\" + 0.012*\"contemplating\" + 0.011*\"od\" + 0.011*\"cigarettes\" + 0.008*\"floor\"\n",
            "178 Topics: 0.013*\"told\" + 0.013*\"mom\" + 0.012*\"him\" + 0.011*\"dad\" + 0.010*\"got\" + 0.010*\"were\" + 0.009*\"didn't\" + 0.009*\"said\" + 0.008*\"went\" + 0.008*\"after\"\n",
            "179 Topics: 0.069*\"cold\" + 0.024*\"horror\" + 0.024*\"apparent\" + 0.018*\"description\" + 0.017*\"game\" + 0.017*\"green\" + 0.016*\"turkey\" + 0.015*\"x-post\" + 0.015*\"approaches\" + 0.015*\"mentality\"\n",
            "180 Topics: 0.020*\"doctor\" + 0.016*\"lexapro\" + 0.013*\"weeks\" + 0.012*\"appointment\" + 0.012*\"taking\" + 0.011*\"day\" + 0.010*\"after\" + 0.009*\"week\" + 0.009*\"started\" + 0.008*\"2\"\n",
            "181 Topics: 0.048*\"p\" + 0.035*\"grad\" + 0.022*\"fantasize\" + 0.019*\"machine\" + 0.017*\"product\" + 0.017*\"asks\" + 0.016*\"bout\" + 0.014*\"rise\" + 0.012*\"provides\" + 0.011*\"population\"\n",
            "182 Topics: 0.030*\"surgery\" + 0.015*\"pain\" + 0.010*\"feel\" + 0.008*\"wisdom\" + 0.008*\"dentist\" + 0.007*\"which\" + 0.007*\"going\" + 0.007*\"worse\" + 0.007*\"procedure\" + 0.006*\"teeth\"\n",
            "183 Topics: 0.046*\"water\" + 0.026*\"shower\" + 0.023*\"smell\" + 0.016*\"tense\" + 0.011*\"sink\" + 0.010*\"anxiety\" + 0.008*\"horrendous\" + 0.008*\"booked\" + 0.006*\"sign\" + 0.006*\"lake\"\n",
            "184 Topics: 0.032*\"positive\" + 0.027*\"wedding\" + 0.025*\"large\" + 0.018*\"weekly\" + 0.014*\"encouragement\" + 0.013*\"small\" + 0.010*\"share\" + 0.008*\"victories\" + 0.007*\"years\" + 0.006*\"engaged\"\n",
            "185 Topics: 0.035*\"routine\" + 0.026*\"sh\" + 0.021*\"room\" + 0.021*\"panicking\" + 0.017*\"bleed\" + 0.015*\"acid\" + 0.015*\"haha\" + 0.013*\"60\" + 0.013*\"stores\" + 0.013*\"pull\"\n",
            "186 Topics: 0.019*\"staring\" + 0.017*\"super\" + 0.017*\"feel\" + 0.017*\"distract\" + 0.016*\"24/7\" + 0.016*\"snapchat\" + 0.014*\"look\" + 0.014*\"techniques\" + 0.013*\"walking\" + 0.013*\"stroke\"\n",
            "187 Topics: 0.015*\"adults\" + 0.012*\"homework\" + 0.011*\"bullying\" + 0.008*\"private\" + 0.007*\"wished\" + 0.007*\"parents\" + 0.007*\"subjects\" + 0.007*\"avoidance\" + 0.006*\"development\" + 0.006*\"school\"\n",
            "188 Topics: 0.268*\"...\" + 0.042*\"....\" + 0.009*\"90\" + 0.009*\"day\" + 0.007*\"breast\" + 0.006*\"it'd\" + 0.006*\"go\" + 0.006*\"got\" + 0.006*\"images\" + 0.005*\"myself\"\n",
            "189 Topics: 0.043*\"fodmap\" + 0.021*\"low\" + 0.018*\"sounding\" + 0.015*\"stage\" + 0.014*\"punching\" + 0.013*\"ungrateful\" + 0.011*\"fibre\" + 0.010*\"nye\" + 0.009*\"horse\" + 0.009*\"neighbours\"\n",
            "190 Topics: 0.020*\"razor\" + 0.015*\"dunno\" + 0.014*\"hardest\" + 0.013*\"blades\" + 0.010*\"kicks\" + 0.008*\"shoes\" + 0.008*\"parents\" + 0.008*\"obligation\" + 0.008*\"duty\" + 0.007*\"school\"\n",
            "191 Topics: 0.030*\"db\" + 0.014*\"mask\" + 0.014*\"journey\" + 0.008*\"cake\" + 0.008*\"nothing's\" + 0.008*\"processes\" + 0.007*\"here\" + 0.007*\"unfamiliar\" + 0.007*\"severity\" + 0.007*\"version\"\n",
            "192 Topics: 0.032*\"hiv\" + 0.032*\"sigh\" + 0.021*\"vague\" + 0.018*\"aids\" + 0.014*\"condom\" + 0.013*\"meat\" + 0.013*\"pen\" + 0.012*\"quote\" + 0.012*\"revealed\" + 0.011*\"stumbled\"\n",
            "193 Topics: 0.105*\"dreams\" + 0.067*\"dream\" + 0.036*\"shaking\" + 0.020*\"ride\" + 0.018*\"vivid\" + 0.013*\"remember\" + 0.012*\"tremors\" + 0.009*\"disturbing\" + 0.008*\"easiest\" + 0.007*\"shaken\"\n",
            "194 Topics: 0.017*\"grief\" + 0.013*\"feel\" + 0.013*\"mindfulness\" + 0.012*\"logically\" + 0.011*\"exhausting\" + 0.010*\"hrs\" + 0.009*\"anxiety\" + 0.008*\"pitched\" + 0.007*\"taste\" + 0.007*\"sentence\"\n",
            "195 Topics: 0.020*\"go\" + 0.018*\"friends\" + 0.012*\"work\" + 0.011*\"going\" + 0.010*\"week\" + 0.010*\"feel\" + 0.010*\"new\" + 0.008*\"first\" + 0.008*\"day\" + 0.007*\"today\"\n",
            "196 Topics: 0.078*\"ï¸\" + 0.020*\"can't\" + 0.020*\"goddamn\" + 0.014*\"fucks\" + 0.014*\"pussy\" + 0.012*\"coma\" + 0.010*\"10pm\" + 0.007*\"myself\" + 0.007*\"feel\" + 0.007*\"motivate\"\n",
            "197 Topics: 0.031*\"gun\" + 0.024*\"cousin\" + 0.018*\"asshole\" + 0.015*\"lab\" + 0.015*\"son\" + 0.013*\"proud\" + 0.012*\"remembered\" + 0.011*\"pile\" + 0.010*\"pool\" + 0.010*\"god\"\n",
            "198 Topics: 0.061*\"anxiety\" + 0.023*\"heart\" + 0.018*\"anxious\" + 0.017*\"feeling\" + 0.016*\"chest\" + 0.015*\"feel\" + 0.009*\"felt\" + 0.009*\"attack\" + 0.008*\"going\" + 0.008*\"over\"\n",
            "199 Topics: 0.029*\"blade\" + 0.020*\"plane\" + 0.020*\"diabetes\" + 0.014*\"europe\" + 0.014*\"airport\" + 0.011*\"flight\" + 0.010*\"burns\" + 0.009*\"thyroid\" + 0.008*\"theater\" + 0.008*\"traveling\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA Generate feature matrix using topic distribution for each Symptom\n",
        "\n",
        "def topic_distribution(lda_model, symptom_posts, dictionary):\n",
        "  \"\"\"\n",
        "  Produces the topic distribution of each post for a symptom.\n",
        "\n",
        "  ROWS: posts\n",
        "  COLUMNS: topics\n",
        "  Values: weight of the topic in the post\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize a feature matrix\n",
        "  M = np.zeros((len(symptom_posts), 200), dtype=np.float64)\n",
        "\n",
        "  # For a specific symptom create a feature matrix\n",
        "  for post_idx, post in enumerate(symptom_posts):\n",
        "\n",
        "    # Convert the post to a bag-of-words representation (uniqueid, count4post)\n",
        "    bow = dictionary.doc2bow(post)\n",
        "    # Use the trained topic model to access the topic distribution for the post\n",
        "    topic_distribution = lda_model[bow]\n",
        "\n",
        "    # Loop through the topic distribution for the post\n",
        "    for topic_id, weight in topic_distribution:\n",
        "      # places the weight of the topic on the row\n",
        "      M[post_idx, topic_id] = weight\n",
        "\n",
        "  return M\n",
        "\n",
        "def feature_matrix(lda_model, processed_data, dictionary):\n",
        "  \"\"\" Produce the topic distribution accross all the symptoms.\"\"\"\n",
        "\n",
        "  lda_features = {}\n",
        "  for symptom, posts in processed_data.items():\n",
        "    print(symptom)\n",
        "\n",
        "    if symptom != 'Depression': # exclude Depression as it is included as other symptoms\n",
        "      M = topic_distribution(lda_model, posts, dictionary)\n",
        "      lda_features[symptom] = M\n",
        "\n",
        "  return lda_features"
      ],
      "metadata": {
        "id": "vRpdGnrrxyii"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST LDA embedding\n",
        "lda_features = feature_matrix(lda_model, processed_data, dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4CdwdLYB_q2",
        "outputId": "3590c54d-8f4c-4f9b-c70f-1820d0fb70ad"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Control\n",
            "Depression\n",
            "Anger\n",
            "Anhedonia\n",
            "Anxiety\n",
            "Concentration deficit\n",
            "Disordered Eating\n",
            "Fatigue\n",
            "Loneliness\n",
            "Sad Mood\n",
            "Self-loathing\n",
            "Sleep problem\n",
            "Somatic complaint\n",
            "Suicidal thoughts and attempts\n",
            "Worthlessness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DOWNLOAD data for future testing\n",
        "with open('lda_features.pkl', 'wb') as f:\n",
        "    pickle.dump(lda_features, f)"
      ],
      "metadata": {
        "id": "WZwGbhO4X0Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RoBERTa Embeddings"
      ],
      "metadata": {
        "id": "E0-97hsVXNkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, tokenizer and device\n",
        "model_name = 'distilroberta-base'\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertModel.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kuf7EKfqNnWf",
        "outputId": "c116f83e-3017-4f3c-9b6d-ed864ce7c021",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "You are using a model of type roberta to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of DistilBertModel were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.sa_layer_norm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertModel(\n",
              "  (embeddings): Embeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (layer): ModuleList(\n",
              "      (0-5): 6 x TransformerBlock(\n",
              "        (attention): MultiHeadSelfAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SymptomDataset(torch.utils.data.Dataset):\n",
        "  \"\"\"\n",
        "  Initialize custom dataloader to load posts in batches of 32. Includes\n",
        "  Roberta tokenizer that pads to the longest sequence in the batch and truncates\n",
        "  to the model's max length.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, posts):\n",
        "      self.posts = posts\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.posts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      return self.posts[idx]  # Return raw post\n",
        "\n",
        "# Custom collate function for padding\n",
        "def collate_fn(batch):\n",
        "    tokenized = tokenizer(\n",
        "        batch,\n",
        "        padding=True,         # Pad to the longest sequence in the batch\n",
        "        truncation=True,      # Truncate sequences longer than the model's max length\n",
        "        return_tensors=\"pt\"   # Return PyTorch tensors\n",
        "    )\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "LQZmgKEE7NSQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Your RoBERTa code!\n",
        "def RoBERTa_embeddings(tokenized_data, layer_num = 5):\n",
        "  \"\"\"\n",
        "  For each symptom produce a embedding by taking the 5th hidden layer\n",
        "  of the roberta model.\n",
        "  \"\"\"\n",
        "  roberta_embeddings = {}\n",
        "\n",
        "  for symptom, posts in tokenized_data.items():\n",
        "    print(symptom)\n",
        "    if symptom != 'Depression':\n",
        "\n",
        "      # produce the training dataset\n",
        "      train_dataset = SymptomDataset(posts)\n",
        "      # load in the model with batches of 32\n",
        "      train_dataloader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "\n",
        "      symptom_embedding_list = [] # list of post embeddings\n",
        "      progress_bar = tqdm(train_dataloader, desc=\"Processing Symptoms\", ncols=100)\n",
        "\n",
        "      for batch in progress_bar:\n",
        "        # needs to be a SINGULAR sentence not a list of tokinized items! made it\n",
        "        # so that embedding was shape (post length, padding_length, embedding_size)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        # access the outputs of the pre-trained model (no training involved)\n",
        "        with torch.no_grad():\n",
        "          outputs = model(input_ids=input_ids, attention_mask = attention_mask, output_hidden_states= True)\n",
        "          hidden_states = outputs.hidden_states # access the models hidden states\n",
        "\n",
        "        # [batch_size, sequence_length, embedding_size]\n",
        "        layer_embeddings = hidden_states[layer_num] # access the 5th layer in the transformer\n",
        "\n",
        "        # [batch_size, embedding_size]\n",
        "        post_embedding = layer_embeddings.mean(dim=1) # get the mean over the sequence length\n",
        "\n",
        "        symptom_embedding_list.append(post_embedding.cpu())\n",
        "\n",
        "    roberta_embeddings[symptom] = symptom_embedding_list\n",
        "\n",
        "  return roberta_embeddings\n"
      ],
      "metadata": {
        "id": "TsNadvvsJDga"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_embeddings = RoBERTa_embeddings(symptom_data, layer_num = 5)"
      ],
      "metadata": {
        "id": "8fCLY66OkHes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DOWNLOAD data for future testing\n",
        "with open('roberta_embeddings_punctuation.pkl', 'wb') as f:\n",
        "    pickle.dump(roberta_embeddings, f)"
      ],
      "metadata": {
        "id": "BXXvlw3CXQGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST roberta_embeddings\n",
        "print(roberta_embeddings['Control']).shape"
      ],
      "metadata": {
        "id": "xFJrIcqNlPzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Embeddings - AUC"
      ],
      "metadata": {
        "id": "rDWxuF2jXtwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LDA_AUC(X, y):\n",
        "  \"\"\"\n",
        "  Runs 5-fold cross validation with random forest to evaluate LDA's embedding\n",
        "  performance.\n",
        "  \"\"\"\n",
        "  rf_classifier = RandomForestClassifier(\n",
        "      max_depth=20, # Limit tree depth to reduce model complexity\n",
        "      min_samples_split=40, # Increase the minimum number of samples required to split a node\n",
        "      min_samples_leaf=20 # Increase the minimum number of samples per leaf node\n",
        "  )\n",
        "  cv = KFold(n_splits=5, shuffle=True)\n",
        "  results = cross_validate(rf_classifier, X=X, y=y, cv=cv, scoring='roc_auc', return_train_score=True)\n",
        "  return np.mean(results['test_score'])\n",
        "\n",
        "def ROBERTA_AUC(X, y):\n",
        "  \"\"\"\n",
        "  Runs 5-fold cross validation with random forest to evaluate LDA's embedding\n",
        "  performance.\n",
        "  \"\"\"\n",
        "  rf_classifier = RandomForestClassifier(\n",
        "    # n_estimators=50,            # Reduce the number of trees to prevent overfitting\n",
        "    max_depth=10,                # Limit tree depth to reduce model complexity\n",
        "    min_samples_split=100,       # Increase the minimum number of samples required to split a node\n",
        "    min_samples_leaf=50        # Increase the minimum number of samples per leaf node\n",
        "  )\n",
        "  cv = KFold(n_splits=5, shuffle=True)\n",
        "  results = cross_validate(rf_classifier, X=X, y=y, cv=cv, scoring='roc_auc', return_train_score=True)\n",
        "  return np.mean(results['test_score'])\n",
        "\n",
        "def evaluate_embeddings(lda_features, roberta_embeddings):\n",
        "  lda_auc_scores = []\n",
        "  roberta_auc_scores = []\n",
        "  symptoms = []\n",
        "\n",
        "  lda_control = lda_features['Control']\n",
        "  roberta_control = np.vstack(roberta_embeddings['Control'])\n",
        "\n",
        "  for symptom in tqdm(lda_features, desc=\"Evaluating LDA model\", ncols=100):\n",
        "      if symptom not in ['Control', 'Depression', 'Concentration deficit', 'Fatigue', 'Suicidal thoughts and attempts']:\n",
        "\n",
        "          lda_symptom_embedding = lda_features[symptom]\n",
        "\n",
        "          # Produces our Input data by concatenating our control and current symptom embeddings\n",
        "          X = np.concatenate((lda_control, lda_symptom_embedding), axis=0)\n",
        "          # Produces our labels by using 0 as control and 1 as symptom \"positive\"\n",
        "          y = np.concatenate((np.zeros(len(lda_control)), np.ones(len(lda_symptom_embedding))), axis=0)\n",
        "\n",
        "          lda_auc = LDA_AUC(X, y)\n",
        "          lda_auc_scores.append(lda_auc)\n",
        "          symptoms.append(symptom)\n",
        "\n",
        "  for symptom in tqdm(roberta_embeddings, desc=\"Evaluating LDA model\", ncols=100):\n",
        "      if symptom not in ['Control', 'Depression', 'Concentration deficit', 'Fatigue', 'Suicidal thoughts and attempts']:\n",
        "\n",
        "          roberta_symptom_embedding = np.vstack(roberta_embeddings[symptom])\n",
        "\n",
        "          # Produces our Input data by concatenating our control and current symptom embeddings\n",
        "          X = np.concatenate((roberta_control, roberta_symptom_embedding), axis=0)\n",
        "          # Produces our labels by using 0 as control and 1 as symptom \"positive\"\n",
        "          y = np.concatenate((np.zeros(len(roberta_control)), np.ones(len(roberta_symptom_embedding))), axis=0)\n",
        "\n",
        "          roberta_auc = ROBERTA_AUC(X, y)\n",
        "          roberta_auc_scores.append(roberta_auc)\n",
        "\n",
        "  # Create DataFrame\n",
        "  auc_data = {\n",
        "      'Symptom': symptoms,\n",
        "      'LDA AUC': lda_auc_scores,\n",
        "      'RoBERTa AUC': roberta_auc_scores,\n",
        "  }\n",
        "  auc_df = pd.DataFrame(auc_data)\n",
        "\n",
        "  # Pretty print the DataFrame using tabulate\n",
        "  table = tabulate(auc_df, headers='keys', tablefmt='fancy_grid', showindex=False)\n",
        "  print(table)\n"
      ],
      "metadata": {
        "id": "nqiZ5G_Ik0Ng"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main\n",
        "Run main, choose if you have already loaded the dateset before or not.\n"
      ],
      "metadata": {
        "id": "hz0C65VbNnUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(load_data = False):\n",
        "\n",
        "  # generate dictionary containing lists of posts\n",
        "\n",
        "  print(\"Generate Symptom Data\")\n",
        "  symptom_data = dataset_generation() # generate our original dictionary\n",
        "  print(\"--------------------------------------\")\n",
        "\n",
        "  print(\"Preprocess our dataset for LDA\\n\")\n",
        "  happy_tokenizer = Tokenizer() # tokenizer already produce lower case token\n",
        "  tokenized_data = tokenize(happy_tokenizer, symptom_data) # tokenize\n",
        "  top_100 = stop_words(tokenized_data) # get top 100 stop words\n",
        "  processed_data = remove_stop_words(tokenized_data, top_100) # remove stop words + lowercases (inherently removes punctuation)\n",
        "\n",
        "  print(\"-------------------------------------\")\n",
        "  # Preload Embeddings\n",
        "  if load_data:\n",
        "    roberta_embeddings, lda_features = load_embeddings()\n",
        "    evaluate_embeddings(lda_features, roberta_embeddings)\n",
        "\n",
        "  else:\n",
        "    # Generate Embeddings\n",
        "    # LDA embeddings\n",
        "    dictionary, corpus = create_corpus(processed_data)\n",
        "    print(\"Training LDA model \\n\")\n",
        "    print(\"-------------------------\")\n",
        "    lda_model = train_lda(dictionary, corpus)\n",
        "    print(\"Produces LDA feature matrix\\n\")\n",
        "    lda_features = feature_matrix(lda_model, processed_data, dictionary)\n",
        "    print(\"-------------------------\")\n",
        "\n",
        "    # RoBERTa embeddings\n",
        "    print(\"Generating Roberta embeddings\\n\")\n",
        "    roberta_embeddings = RoBERTa_embeddings(symptom_data)\n",
        "    print(\"-------------------------\")\n",
        "\n",
        "    print(\"Evaluating models\\n\")\n",
        "    # Produces a table that represents the AUC scores of both the LDA and roberta embeddings\n",
        "    evaluate_embeddings(lda_features, roberta_embeddings)\n",
        "\n",
        "main(True)"
      ],
      "metadata": {
        "id": "IN4OhA5kzx_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f4f1e7-2869-4312-c1e3-ff5c1f6c704d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate Symptom Data\n",
            "--------------------------------------\n",
            "Preprocess our dataset for LDA\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing Symptoms: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:22<00:00,  9.49s/it]\n",
            "Removing Stop Words: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:29<00:00,  1.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚ Symptom           â”‚   LDA AUC â”‚   RoBERTa AUC â”‚\n",
            "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ Anger             â”‚  0.949425 â”‚      0.892395 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Anhedonia         â”‚  0.965538 â”‚      0.909087 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Anxiety           â”‚  0.937916 â”‚      0.892172 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Disordered Eating â”‚  0.961068 â”‚      0.87232  â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Loneliness        â”‚  0.879236 â”‚      0.825268 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Sad Mood          â”‚  0.844092 â”‚      0.845463 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Self-loathing     â”‚  0.875498 â”‚      0.85112  â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Sleep problem     â”‚  0.976371 â”‚      0.882789 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Somatic complaint â”‚  0.916296 â”‚      0.815476 â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Worthlessness     â”‚  0.738913 â”‚      0.786843 â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ethical Discussion\n",
        "\n",
        "### Benefits:\n",
        "- NLP systems process large amounts of data --> can produce insights on mental health and symptom patterns\n",
        "- Early Warning Systems\n",
        "\n",
        "### Drawbacks:\n",
        "- NLP is still not human <-- can still lack contextual understanding\n",
        "- Mental health is unique to individuals <-- data can have biases to certain demographics depending on where we access the data\n",
        "- Mental health is extremely complicated we shouldn't over-rely on nlp for diagnoses\n",
        "\n",
        "### Harms:\n",
        "- Mining user data can always be extremely dangerous from a privacy perspective.\n",
        "- False positives and negatives can have serious real world consequences on peoples lives.\n",
        "- Could be miss used for example targeted advertising.\n"
      ],
      "metadata": {
        "id": "qGo0RurjicgL"
      }
    }
  ]
}